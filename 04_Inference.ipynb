{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ead4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\funasr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import onnxruntime\n",
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "from llama_cpp import (\n",
    "    Llama,\n",
    "    llama_batch_init,\n",
    "    llama_batch_free,\n",
    "    llama_decode,\n",
    "    llama_get_logits,\n",
    "    llama_kv_self_clear,  # 新版 API：清理缓存\n",
    ")\n",
    "\n",
    "# =========================================================================\n",
    "# 配置部分\n",
    "# =========================================================================\n",
    "\n",
    "# 日志设置\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"inference_refine.log\", encoding='utf-8', mode='w'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 模型路径\n",
    "model_dir = r'./model-gguf'\n",
    "tokenizer_path = f'{model_dir}/Qwen3-0.6B'\n",
    "\n",
    "# ONNX 模型\n",
    "onnx_encoder = f'{model_dir}/FunASR_Nano_Encoder.onnx'       # Audio Encoder\n",
    "\n",
    "# GGUF 模型 (用于解码)\n",
    "gguf_model_path = f'{model_dir}/qwen3-0.6b-asr.gguf'\n",
    "\n",
    "\n",
    "# Prompt 配置 (Prefix + Audio + Suffix)\n",
    "# 注意：Audio Encoder 已经移除了硬编码的 Prompt，现在需要我们手动拼接。\n",
    "PREFIX_PROMPT = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "SUFFIX_PROMPT = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "# 音频处理参数 (需与模型训练时一致)\n",
    "SAMPLE_RATE = 16000\n",
    "USE_NORMALIZER = True\n",
    "MAX_INPUT_AUDIO_LENGTH = 320000 \n",
    "SLIDING_WINDOW = 0 # 0 表示根据音频长度自动分段 (这里简化为一次性处理或整段)\n",
    "\n",
    "# 模型参数\n",
    "MAX_SEQ_LEN = 1024\n",
    "STOP_TOKEN = [151643, 151645] # Qwen 的特殊停止 Token\n",
    "MAX_THREADS = 0 # 0 = Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e772122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================================\n",
    "# 辅助函数\n",
    "# =========================================================================\n",
    "\n",
    "def load_gguf_embeddings(model_path):\n",
    "    \"\"\"Load the raw token embedding table from the GGUF file.\"\"\"\n",
    "    # 尝试加载 gguf 库\n",
    "    gguf_py_path = os.path.abspath(\"./llama-cpp-python/vendor/llama.cpp/gguf-py\")\n",
    "    if gguf_py_path not in sys.path:\n",
    "        sys.path.append(gguf_py_path)\n",
    "    \n",
    "    try:\n",
    "        from gguf import GGUFReader\n",
    "    except ImportError:\n",
    "        logger.error(\"Could not import GGUFReader. Please ensure ./llama-cpp-python/vendor/llama.cpp/gguf-py exists.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger.info(f\"Reading GGUF tensors from {model_path}...\")\n",
    "    reader = GGUFReader(model_path, 'r')\n",
    "    \n",
    "    try:\n",
    "        tensor = next(t for t in reader.tensors if t.name == \"token_embd.weight\")\n",
    "    except StopIteration:\n",
    "        logger.error(\"Could not find 'token_embd.weight' in GGUF file.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    weights = np.array(tensor.data).reshape(-1, 1024) # 假设 Hidden Size 为 1024，需确保与模型一致\n",
    "    logger.info(f\"Loaded Embeddings: Shape={weights.shape}, Type={weights.dtype}\")\n",
    "    \n",
    "    if weights.dtype != np.float32:\n",
    "        logger.info(\"Converting embeddings from float16/other to float32...\")\n",
    "        weights = weights.astype(np.float32)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def normalizer(_audio, target_value=8192.0):\n",
    "    \"\"\"音频归一化处理\"\"\"\n",
    "    _audio = _audio.astype(np.float32)\n",
    "    rms = np.sqrt(np.mean((_audio * _audio), dtype=np.float32), dtype=np.float32)\n",
    "    _audio *= (target_value / (rms + 1e-7))\n",
    "    np.clip(_audio, -32768.0, 32767.0, out=_audio)\n",
    "    return _audio.astype(np.int16)\n",
    "\n",
    "def decode_with_pure_embeddings(llm_obj, audio_embeddings, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    纯 Embedding 解码函数 (Pure Embedding Decoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 准备数据\n",
    "    embeds = audio_embeddings.squeeze()\n",
    "    if len(embeds.shape) == 1:\n",
    "        embeds = embeds.reshape(1, -1)\n",
    "    \n",
    "    n_tokens, n_dim = embeds.shape\n",
    "    logger.info(f\"注入 Total Embeddings Shape: {embeds.shape}\")\n",
    "\n",
    "    # 2. 初始化 Batch\n",
    "    batch_embd = llama_batch_init(n_tokens, n_dim, 1)        \n",
    "    \n",
    "    # batch_text: 用于存放生成的 Token IDs\n",
    "    batch_text = llama_batch_init(1, 0, 1)\n",
    "\n",
    "    ctx = llm_obj.ctx\n",
    "    \n",
    "    # 3. 清理上下文缓存 (KV Cache)\n",
    "    llama_kv_self_clear(llm_obj.ctx) \n",
    "    \n",
    "    try:\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 阶段 A: 注入融合 Embedding\n",
    "        # ---------------------------------------------------------------------\n",
    "        logger.info(\"正在注入融合 Embedding...\")\n",
    "        \n",
    "        batch_embd.n_tokens = n_tokens\n",
    "        llm_obj.n_tokens = 0 # 重置 LLM 内部计数器\n",
    "        \n",
    "        # 关键：batch.token 设置为 NULL，告知底层使用 embedding\n",
    "        batch_embd.token = ctypes.cast(None, ctypes.POINTER(ctypes.c_int32))\n",
    "\n",
    "        for i in range(n_tokens):\n",
    "            batch_embd.pos[i] = i\n",
    "            batch_embd.n_seq_id[i] = 1\n",
    "            batch_embd.seq_id[i][0] = 0\n",
    "            \n",
    "            # 只在最后一个 Token 开启 Logits 计算\n",
    "            batch_embd.logits[i] = 1 if i == n_tokens - 1 else 0\n",
    "\n",
    "        # 使用 ctypes.memmove 高效拷贝 Numpy 数据到 C 指针\n",
    "        if not embeds.flags['C_CONTIGUOUS']:\n",
    "            embeds = np.ascontiguousarray(embeds)\n",
    "        \n",
    "        ctypes.memmove(batch_embd.embd, embeds.ctypes.data, embeds.nbytes)\n",
    "        \n",
    "        # 执行解码\n",
    "        if llama_decode(ctx, batch_embd) != 0:\n",
    "             raise RuntimeError(\"Audio embedding decoding failed\")\n",
    "        \n",
    "        llm_obj.n_tokens += n_tokens\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 阶段 B: 文本生成 (Greedy Search)\n",
    "        # ---------------------------------------------------------------------\n",
    "        generated_text = \"\"\n",
    "        logger.info(f\"开始生成文本 (最大 {max_new_tokens} tokens)...\\n\")\n",
    "        \n",
    "        eos_token = llm_obj.token_eos()\n",
    "        vocab_size = llm_obj.n_vocab()\n",
    "        \n",
    "        batch_text.n_tokens = 1\n",
    "        \n",
    "        gen_start_time = time.time()\n",
    "        tokens_generated = 0\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            # 1. 获取 Logits\n",
    "            logits_ptr = llama_get_logits(ctx)\n",
    "            logits_arr = np.ctypeslib.as_array(logits_ptr, shape=(vocab_size,))\n",
    "            \n",
    "            # 2. 贪婪采样 (Argmax)\n",
    "            token_id = int(np.argmax(logits_arr))\n",
    "            \n",
    "            # 3. 检查停止条件\n",
    "            if token_id == eos_token or token_id in STOP_TOKEN:\n",
    "                break\n",
    "                \n",
    "            # 4. 解码 token 为文本\n",
    "            try:\n",
    "                text_piece = llm_obj.detokenize([token_id]).decode('utf-8', errors='ignore')\n",
    "                print(text_piece, end=\"\", flush=True)\n",
    "                generated_text += text_piece\n",
    "                tokens_generated += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "            # 5. 把生成的 Token 喂回去 (Autoregressive)\n",
    "            batch_text.token[0] = token_id\n",
    "            batch_text.pos[0] = llm_obj.n_tokens\n",
    "            batch_text.n_seq_id[0] = 1\n",
    "            batch_text.seq_id[0][0] = 0\n",
    "            batch_text.logits[0] = 1\n",
    "            \n",
    "            if llama_decode(ctx, batch_text) != 0:\n",
    "                break\n",
    "            \n",
    "            llm_obj.n_tokens += 1\n",
    "            \n",
    "        print('\\n\\n')\n",
    "        gen_duration = time.time() - gen_start_time\n",
    "        tps = tokens_generated / gen_duration if gen_duration > 0 else 0\n",
    "\n",
    "        logger.info(f\"解码速度: {tps:.2f} tokens/s ({tokens_generated} tokens in {gen_duration:.2f}s)\\n\\n\")\n",
    "        \n",
    "    finally:\n",
    "        # 释放资源\n",
    "        llama_batch_free(batch_embd)\n",
    "        llama_batch_free(batch_text)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6d4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Refactored Inference Engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './model-gguf/Qwen3-0.6B' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading GGUF model: ./model-gguf/qwen3-0.6b-asr.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "2026-01-19 17:28:21,563 - INFO - Reading GGUF tensors from ./model-gguf/qwen3-0.6b-asr.gguf...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGUF model loaded successfully!\n",
      "\n",
      "Loading Raw Embeddings from GGUF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:28:26,229 - INFO - Loaded Embeddings: Shape=(151936, 1024), Type=float16\n",
      "2026-01-19 17:28:26,230 - INFO - Converting embeddings from float16/other to float32...\n",
      "2026-01-19 17:28:26,508 - INFO - Prefix Shape: (14, 1024), Suffix Shape: (5, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ONNX Audio Encoder...\n"
     ]
    }
   ],
   "source": [
    "print('\\nStarting Refactored Inference Engine...')\n",
    "    \n",
    "# 1. 初始化 tokenizer (仅用于将 Prefix/Suffix 转为 ID，以便查表)\n",
    "# 注意：需确保 tokenizer 与 GGUF 模型词表一致\n",
    "# 这里的 tokenizer 是 Python 端的，用于预处理\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# 2. 加载 GGUF 模型 (用于解码)\n",
    "print(f'\\nLoading GGUF model: {gguf_model_path}')\n",
    "llm = Llama(\n",
    "    model_path=gguf_model_path,\n",
    "    n_ctx=MAX_SEQ_LEN + 1024, # 适当增加 Context 窗口\n",
    "    n_threads=MAX_THREADS,\n",
    "    embedding=True, # 必须开启\n",
    "    verbose=False\n",
    ")\n",
    "print('GGUF model loaded successfully!')\n",
    "\n",
    "# 3. 从 GGUF 文件中加载 Raw Embeddings\n",
    "print('\\nLoading Raw Embeddings from GGUF...')\n",
    "all_embeddings = load_gguf_embeddings(gguf_model_path)\n",
    "\n",
    "# 4. 准备 Prefix 和 Suffix 的 Embeddings\n",
    "# 必须启用 special tokens 才能正确编码 <|im_start|> 等\n",
    "prefix_tokens = tokenizer.encode(PREFIX_PROMPT, add_special_tokens=True) \n",
    "suffix_tokens = tokenizer.encode(SUFFIX_PROMPT, add_special_tokens=True)\n",
    "\n",
    "prefix_emb = all_embeddings[prefix_tokens]\n",
    "suffix_emb = all_embeddings[suffix_tokens]\n",
    "\n",
    "logger.info(f\"Prefix Shape: {prefix_emb.shape}, Suffix Shape: {suffix_emb.shape}\")\n",
    "\n",
    "# 5. 初始化 ONNX Audio Encoder\n",
    "print('\\nLoading ONNX Audio Encoder...')\n",
    "session_opts = onnxruntime.SessionOptions()\n",
    "session_opts.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session_opts.intra_op_num_threads = MAX_THREADS\n",
    "ort_session_A = onnxruntime.InferenceSession(onnx_encoder, sess_options=session_opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "in_name_A = [x.name for x in ort_session_A.get_inputs()]\n",
    "out_name_A = [x.name for x in ort_session_A.get_outputs()]\n",
    "shape_value_in_A = ort_session_A._inputs_meta[0].shape[-1]\n",
    "\n",
    "# Query Embed for Audio Encoder input (Original logic kept this input)\n",
    "# 假设 hidden_size = 1024\n",
    "query_embed_input = np.ones((1, 10, 1024), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8c1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test Input Audio: ./input.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:29:07,689 - INFO - 注入 Total Embeddings Shape: (155, 1024)\n",
      "2026-01-19 17:29:07,706 - INFO - 正在注入融合 Embedding...\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fused embedding to pickles/embedding_slice_0_160000.pkl\n",
      "\n",
      "=== 推理切片 [0:160000] ===\n",
      "Final Input Shape: (155, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:29:08,685 - INFO - 开始生成文本 (最大 1024 tokens)...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，星期日，欢迎收看一千零四期誓言消息，请静静介绍话题。去年十月十九日，九百六十七期节目说到委内瑞拉问题，我们回顾一下你当时的评。\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:29:09,859 - INFO - 解码速度: 38.35 tokens/s (45 tokens in 1.17s)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 识别\n",
    "\n",
    "# 输入音频\n",
    "test_audio = r'./input.mp3'\n",
    "\n",
    "\n",
    "# 6. 处理音频并推理\n",
    "for audio_file in [test_audio]:\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Test Input Audio: {audio_file}\")\n",
    "    \n",
    "    # 加载和归一化音频\n",
    "    audio = np.array(AudioSegment.from_file(audio_file).set_channels(1).set_frame_rate(SAMPLE_RATE).get_array_of_samples(), dtype=np.int16)\n",
    "    if USE_NORMALIZER:\n",
    "        audio = normalizer(audio, 8192.0)\n",
    "        \n",
    "    audio_len = len(audio)\n",
    "    audio = audio.reshape(1, 1, -1)\n",
    "    \n",
    "    # 定义输入长度\n",
    "    if isinstance(shape_value_in_A, str):\n",
    "            INPUT_AUDIO_LENGTH = min(MAX_INPUT_AUDIO_LENGTH, audio_len)\n",
    "    else:\n",
    "            INPUT_AUDIO_LENGTH = shape_value_in_A\n",
    "            \n",
    "    stride_step = INPUT_AUDIO_LENGTH if SLIDING_WINDOW <= 0 else SLIDING_WINDOW\n",
    "    \n",
    "    # Padding\n",
    "    if audio_len < INPUT_AUDIO_LENGTH:\n",
    "            pad_len = INPUT_AUDIO_LENGTH - audio_len\n",
    "            pad_samples = np.zeros((1, 1, pad_len), dtype=audio.dtype)\n",
    "            audio = np.concatenate((audio, pad_samples), axis=-1)\n",
    "            \n",
    "    aligned_len = audio.shape[-1]\n",
    "    \n",
    "    asr_result = \"\"\n",
    "    slice_start = 0\n",
    "    slice_end = INPUT_AUDIO_LENGTH\n",
    "    \n",
    "    while slice_end <= aligned_len:\n",
    "        # 6.1 运行 ONNX Audio Encoder\n",
    "        input_feed_A = {}\n",
    "        input_feed_A[in_name_A[0]] = onnxruntime.OrtValue.ortvalue_from_numpy(audio[..., slice_start: slice_end], 'cpu', 0)\n",
    "        input_feed_A[in_name_A[1]] = onnxruntime.OrtValue.ortvalue_from_numpy(query_embed_input, 'cpu', 0)\n",
    "        \n",
    "        all_outputs_A = ort_session_A.run_with_ort_values(out_name_A, input_feed_A)\n",
    "        \n",
    "        # audio_features: (1, Seq_Len, 1024)\n",
    "        # 现在 Encoder 返回: [query_embed, x]\n",
    "        audio_features = all_outputs_A[0].numpy()\n",
    "        \n",
    "        # 6.2 拼接: [Prefix, Audio, Suffix]\n",
    "        # 注意: audio_features 是 (1, N, 1024)，prefix/suffix 是 (N, 1024)\n",
    "        # 统一成 (N, 1024)\n",
    "        audio_features_sq = audio_features.squeeze(0) \n",
    "        \n",
    "        final_embedding = np.concatenate([prefix_emb, audio_features_sq, suffix_emb], axis=0)\n",
    "        \n",
    "        # Save to pickle\n",
    "        os.makedirs(\"pickles\", exist_ok=True)\n",
    "        pkl_path = f\"pickles/embedding_slice_{slice_start}_{slice_end}.pkl\"\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(final_embedding, f)\n",
    "        print(f\"Saved fused embedding to {pkl_path}\")\n",
    "\n",
    "        print(f\"\\n=== 推理切片 [{slice_start}:{slice_end}] ===\")\n",
    "        print(f\"Final Input Shape: {final_embedding.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # 6.3 调用 LLM 解码\n",
    "            result_text = decode_with_pure_embeddings(\n",
    "                llm, \n",
    "                final_embedding,\n",
    "                max_new_tokens=MAX_SEQ_LEN\n",
    "            )\n",
    "            asr_result += result_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"解码发生错误: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        slice_start += stride_step\n",
    "        slice_end = slice_start + INPUT_AUDIO_LENGTH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
